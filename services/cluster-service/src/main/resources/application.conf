# HORTONWORKS DATAPLANE SERVICE AND ITS CONSTITUENT SERVICES
# (c) 2016-2018 Hortonworks, Inc. All rights reserved.
# This code is provided to you pursuant to your written agreement with Hortonworks, which may be the terms of the
# Affero General Public License version 3 (AGPLv3), or pursuant to a written agreement with a third party authorized
# to distribute this code.  If you do not have a written agreement with Hortonworks or with an authorized and
# properly licensed third party, you do not have any rights to this code.
# If this code is provided to you under the terms of the AGPLv3:
#   (A) HORTONWORKS PROVIDES THIS CODE TO YOU WITHOUT WARRANTIES OF ANY KIND;
# (B) HORTONWORKS DISCLAIMS ANY AND ALL EXPRESS AND IMPLIED WARRANTIES WITH RESPECT TO THIS CODE, INCLUDING BUT NOT
# LIMITED TO IMPLIED WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE;
# (C) HORTONWORKS IS NOT LIABLE TO YOU, AND WILL NOT DEFEND, INDEMNIFY, OR HOLD YOU HARMLESS FOR ANY CLAIMS ARISING
# FROM OR RELATED TO THE CODE; AND
# (D) WITH RESPECT TO YOUR EXERCISE OF ANY RIGHTS GRANTED TO YOU FOR THE CODE, HORTONWORKS IS NOT LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, PUNITIVE OR CONSEQUENTIAL DAMAGES INCLUDING, BUT NOT LIMITED TO,
# DAMAGES RELATED TO LOST REVENUE, LOST PROFITS, LOSS OF INCOME, LOSS OF BUSINESS ADVANTAGE OR UNAVAILABILITY,
# OR LOSS OR CORRUPTION OF DATA.
#Application configuration

env = "production"

#dp-keystore settings
dp.keystore.path=${PWD}/dp-build/shared/dp-keystore.jceks
dp.keystore.password=changeit

# Data plane creates its own store in the base directory
dp.services.keystore.base.path=${HOME}
dp.services.keystore.path=".dp_keystore"
dp.services.keystore.name="dp_certs_store"
dp.services.keystore.pass="P92[W%CP#d7g@^E8"
dp.services.keystore.removeOnExit=false

dp.services.ssl.config {
  disable.hostname.verification = false
}

dp.services.db {
  service.uri = "http://localhost:9005"
  service.path = "/service/db"
}

dp.service.hdinsight {
  auth.status = 401
  auth.response.message = "Probable HD-Insight cluster, Basic authentication detected"
  auth.challenge.contentType="text/html"
  auth.challenge.wwwAuthenticate="Basic realm=HDInsight"
}

dp.service.ambari {
  api.version = "/api/v1"
  cluster.api.prefix = "/api/v1/clusters"
  stack.api.prefix = "/api/v1/stacks"
  status.check.timeout.secs = 180

  # Set this to true if Ambari+HDP services are all on the same node
  # Underlying services will default to using the host/ip provided as the Ambari URL
  single.node.cluster="false"

}

# in Seconds
dp.services.ws.client.requestTimeout.mins = 4

# in Seconds
dp.services.cluster.sync.start.secs = 10
# in Minutes
dp.services.cluster.sync.interval.mins = 5
#Bind address for server
dp.services.cluster.http.host = "0.0.0.0"
#Port for server
dp.services.cluster.http.port = 9009

#Time to cache the API for the cluster
dp.services.cluster.http.atlas.api.cache.secs = 3600

# Time to memoize the URL supplier for each cluster
dp.services.cluster.atlas.proxy.cache.expiry.secs = 86400

#Time to expire the cached Atlas Api client
dp.services.cluster.http.atlas.token.cache.secs = 3600

dp.services.cluster.knox.token.cache.expiry.secs = 3600

#How long before token expiry must the token cache key be invalidated - in seconds
dp.services.cluster.knox.token.cache.removal.time = 3


#Http client/server settings
akka.http {
  server.request-timeout=180s
  client.connecting-timeout=180s
  parsing {
    max-uri-length = 16k
  }
}
play.ws.timeout {
  connection=180s
  request=180s
}

dp.services.hdp_proxy {

  host = "0.0.0.0"
  port = "9010"
  consul {
    #enabled = false
    #unique name
    serviceId = "hdp_proxy_01"
    #common name across instances
    serviceName = "hdp_proxy"
    service.tags = ["cluster-proxy-service"]
    service.port = 9010
    client.connect.failure.retry.secs = 5
    host = "localhost"
    port = 8500
  }

  #Only these services will be allowed to use the knox gateway , for all other services
  # the proxy will use the target URL specified in the header
  services = [
    "webhdfs",
    "atlas",
    "beacon",
    "ranger",
    "profiler-agent",
    "smm",
    "logsearch"
  ]

  service.uri = "http://localhost:9010"
  service.path = "/service/hdp"

}


dp.services.atlas {

  atlas.common.attributes = [
    {"name": "owner", "dataType": "string"},
    {"name": "name", "dataType": "string"},
    {"name": "db.name", "dataType": "string"},
    {"name": "tag", "dataType": "tag"}
  ]

  #Types to be accepted
  hive.accepted.types = ["string", "int", "long", "boolean", "date"]

  hive.search.query.base = "hive_table"

  lower.case.queries = false

  filter.deleted.entities = true

}


#there is no good way to find these out ,
# it is assumed that these values will be used by convention
dp.services.knox {

  # Name for the token topology
  token.topology="token"

  # Name for the redirect token topology. This is used to get the short lived token for redirecting
  # to a service to enable SSO
  token.redirect.topology="redirecttoken"

  #Name of the dp sso topology, pointed to by the token
  token.target.topology="dp-proxy"

  # Name of the redirect SSO topology pointer to by the token
  token.redirect.target.topology="redirect"

  #Should this service instance expect a sperate knox config group
  # ITS IMPORTANT THAT ALL SERVICE INSTANCES (IN HA) HAVE THE SAME VALUE FOR THIS CONFIGURATION
  token.expect.separate.config=false

  #Name of the config group
  token.config.group.name="dataplane"

  #Which mechanism should be used for inferring the additional config group and the final knox URL
  #if true then ambari credentials are used, if false then knox url is expected
  token.infer.endpoint.using.credentials=true

  #Name of the gateway path eg: https://ip:port/<gateway.name>
  gateway.name = "gateway"

  #Set to ture if gateway name must be inferred from the provider URL
  infer.gateway.name=true


}

DPSPlatform.credential {
  ambari {
    username = "admin"
    password = "admin"
  }
  atlas {
    username = "admin"
    password = "admin"
  }
  ranger {
    username = "admin"
    password = "admin"
  }
}

consul {
  #enabled = false
  #unique name
  serviceId = "clusters_01"
  #common name across instances
  serviceName = "clusters"
  service.tags = ["cluster-service"]
  service.port = 9009
  client.connect.failure.retry.secs = 5
  host = "localhost"
  port = 8500
}

gateway {
  ssl.enabled = false
  refresh.servers.secs = 60
}

atlas.query.records.default {
  limit = 10000
  offset = 0
}

dp.certificate {
  query.timeout = "4 minutes"
  algorithm.blacklist.key = [
    "RSA keySize < 1024"
  ]
  algorithm.blacklist.signature = [
    "RSA keySize < 1024"
  ]
}


additional.services:[
  {
    service:"KAFKA", component: "KAFKA_BROKER"
  },{
    service:"STREAMSMSGMGR", component: "STREAMSMSGMGR"
  },{
    service:"ZOOKEEPER", component: "ZOOKEEPER_SERVER"
  },{
    service:"AMBARI_METRICS", component: "METRICS_GRAFANA"
  },{
    service: "ATLAS", component: "ATLAS_SERVER"
  }, {
    service: "DATA_ANALYTICS_STUDIO", component: "DATA_ANALYTICS_STUDIO_WEBAPP"
  }, {
    service: "BEACON", component: "BEACON_SERVER"
  }, {
    service: "HIVE", component: "HIVE_SERVER"
  }, {
    service: "DPPROFILER", component: "DP_PROFILER_AGENT"
  }, {
    service: "RANGER", component: "RANGER_ADMIN"
  }, {
    service: "KNOX", component: "KNOX_GATEWAY"
  }, {
    service: "YARN", component: "RESOURCEMANAGER"
  }, {
    service: "LOGSEARCH", component: "LOGSEARCH_SERVER"
  }
]
